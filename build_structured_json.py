"""Produce structured JSON (one per extractor) for a given PDF stem.

Structure matches requested schema:
{
  "ΑΔΑ": <stem or detected>,
  "ΚΑΕΚ": <kaek>,
  "Στοιχεία κυρίου του έργου": [ { "Επώνυμο/ία": ..., "Όνομα": ..., ... } ],
  "Στοιχεία Διαγράμματος Κάλυψης": {
      "ΥΦΙΣΤΑΜΕΝΑ": { row: value, ... },
      "ΝΟΜΙΜΟΠΟΙΟΥΜΕΝΑ": {...},
      "ΠΡΑΓΜΑΤΟΠΟΙΟΥΜΕΝΑ": {...},
      "ΣΥΝΟΛΟ": {...}
  }
}

Input sources:
    A) debug/compare/<stem>_pdfplumber.txt and *_docling.txt (generated by compare_pdf_extractors.py --save-text)
    B) Or directly from a PDF via --pdf and selecting engines with --extractors

Usage:
    # From pre-extracted texts (both engines by default)
    python build_structured_json.py --stem 63ΤΛ46Ψ842-Ξ2Μ

    # Directly from a PDF (choose engine)
    python build_structured_json.py --pdf data/Athens/63ΤΛ46Ψ842-Ξ2Μ.pdf --extractors pdfplumber
Outputs:
    debug/structured_json/<stem>_pdfplumber_structured.json
    debug/structured_json/<stem>_docling_structured.json
"""
from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Optional

COMPARE_DIR = Path("debug/compare")

COVERAGE_KEYS = [
    "Εμβ. κάλυψης κτιρίου",
    "Εμβ. δόμησης κτιρίου",
    "Εμβ. ακάλυπτου χώρου οικοπέδου",
    "Όγκος κτιρίου (άνω εδάφους)",
    "Μέγιστο ύψος κτιρίου",
    "Αριθμός Ορόφων",
    "Αριθμός Θέσεων Στάθμευσης",
]

EU_NUM_RE = re.compile(r"^-?[0-9][0-9\.]*,[0-9]+$|^-?[0-9][0-9\.]*$")

def parse_eu_number(s: str) -> Optional[float]:
    """Parse a European-formatted number with support for optional leading minus.

    Rules:
      - Comma (,) is treated as decimal separator.
      - Dots (.) are thousands separators when a comma exists, OR when there are >=2 groups of 3 digits separated by dots.
      - When no comma exists and exactly one dot is present and the group after the dot has length 3, treat dot as thousands (e.g. '1.234' -> 1234) NOT decimal.
      - When no comma exists and a single dot is present with 1-2 trailing digits, treat as decimal (rare but allow if pattern clearly decimal like '7.5').
    """
    raw = s
    s = s.strip().replace(" ", "")
    if not s or s in {"-", "--"}:
        return None
    sign = -1 if s.startswith('-') else 1
    if s[0] in '+-':
        s_body = s[1:]
    else:
        s_body = s
    if not s_body:
        return None
    if ',' in s_body:
        int_part, dec = s_body.split(',', 1)
        int_part = int_part.replace('.', '')
        num_str = int_part + '.' + dec
    else:
        # No comma. Decide how to interpret dots.
        if s_body.count('.') == 0:
            num_str = s_body
        else:
            parts = s_body.split('.')
            if len(parts) > 1 and all(len(p) == 3 for p in parts[1:]):
                # All subsequent groups length 3: treat all dots as thousands
                num_str = ''.join(parts)
            elif len(parts) == 2 and len(parts[1]) == 3:
                # Single dot then 3 digits -> thousands
                num_str = parts[0] + parts[1]
            elif len(parts) == 2 and 1 <= len(parts[1]) <= 2:
                # Looks like a decimal
                num_str = parts[0] + '.' + parts[1]
            else:
                # Fallback: strip all dots (thousands assumption)
                num_str = ''.join(parts)
    try:
        return sign * float(num_str)
    except ValueError:
        return None

def load_raw(stem: str) -> Dict[str, str]:
    pdfpl = (COMPARE_DIR / f"{stem}_pdfplumber.txt").read_text(encoding="utf-8")
    docling = (COMPARE_DIR / f"{stem}_docling.txt").read_text(encoding="utf-8")
    return {"pdfplumber": pdfpl, "docling": docling}

# --- Direct text extraction helpers (for programmatic use or --pdf flow) ---
def _extract_text_pdfplumber(pdf_path: Path) -> str:
    import pdfplumber  # type: ignore
    import re as _re
    with pdfplumber.open(str(pdf_path)) as pdf:
        pages = [p.extract_text() or "" for p in pdf.pages]
    text = "\n\n".join(pages)
    text = _re.sub(r"(\w)-\n(\w)", r"\1\2", text)
    text = _re.sub(r"[ \t]+", " ", text)
    text = _re.sub(r"\n{2,}", "\n\n", text)
    return text.strip()

def _extract_text_docling(pdf_path: Path) -> Optional[str]:
    try:
        from docling.document_converter import DocumentConverter  # type: ignore
    except Exception:
        return None
    converter = DocumentConverter()
    result = converter.convert(str(pdf_path))
    text = result.document.export_to_text() if hasattr(result.document, "export_to_text") else str(result.document)
    return (text or "").strip()

def extract_pdf_to_structured(pdf_path: Path, extractor: str) -> Dict[str, object]:
    """High-level helper: extract one PDF using a single engine into structured JSON.

    extractor: 'pdfplumber' or 'docling'
    Returns the structured dict as produced by build_structured().
    """
    extractor = extractor.lower().strip()
    if extractor == "pdfplumber":
        raw_text = _extract_text_pdfplumber(pdf_path)
    elif extractor == "docling":
        raw_text = _extract_text_docling(pdf_path)
        if raw_text is None:
            raise RuntimeError("docling not installed or failed to import")
    else:
        raise ValueError("extractor must be 'pdfplumber' or 'docling'")
    return build_structured(pdf_path.stem, raw_text, extractor)

def extract_kaek(text: str) -> Optional[str]:
    """Extract ΚΑΕΚ value from raw text or markdown table rows.

    Handles patterns:
      ΚΑΕΚ <spaces> 0500...
      | ΚΑΕΚ | 0500... |
      ΚΑΕΚ : 0500...
    """
    # Direct simple pattern
    m = re.search(r"ΚΑΕΚ\s*[:\-]?\s*([0-9/]{6,})", text)
    if m:
        return m.group(1)
    # Scan line by line for table formatted rows
    for line in text.splitlines():
        if "ΚΑΕΚ" in line:
            m2 = re.search(r"ΚΑΕΚ[^0-9/]*([0-9/]{6,})", line)
            if m2:
                return m2.group(1)
    return None

def _post_process_kaek(raw_text: str, value: str) -> str:
    """Heuristically fix common KAEK extraction issues.

    Fixes implemented:
      1. Reconstruct missing '/0/0' when the source line clearly contains separated '/ 0 / 0'.
      2. Remove spurious leading zero for 11-digit numeric KAEK codes (e.g. '050097350003' -> '50097350003'),
         only when there are no slashes and pattern matches 0 + 5 + 10 digits.
      3. If no value yet, attempt secondary scan combining fragmented table cells where number appears before 'ΚΑΕΚ'.
    """
    # Helper to collapse internal spaces around slashes
    def tidy(v: str) -> str:
        v = v.replace(' /', '/').replace('/ ', '/').replace(' ', '')
        v = re.sub(r'/+', '/', v)
        return v

    original = value or ""

    # Secondary scan if empty: look for pattern like '([0-9]{6,}) / 0 / 0' or reversed table row
    if not original:
        for line in raw_text.splitlines():
            if 'ΚΑΕΚ' in line or re.search(r'[0-9]{6,}\s*/\s*0\s*/\s*0', line):
                m = re.search(r'([0-9]{6,})\s*/\s*0\s*/\s*0', line)
                if m:
                    original = m.group(1) + '/0/0'
                    break
                # Reversed orientation: numeric then '|' then 'ΚΑΕΚ'
                if 'ΚΑΕΚ' in line:
                    # collect all pure numeric >=6 chars in line
                    nums = re.findall(r'[0-9]{6,}', line)
                    if nums:
                        # choose longest
                        original = max(nums, key=len)
                        # Special fragmented pattern: we saw '/ 0 /' but final 0 is after 'ΚΑΕΚ'
                        # Case A: suffix fully fragmented across cells -> first cell ends with '/ 0 /' then a pipe, later cell has 'ΚΑΕΚ 0'
                        # Case B: earlier heuristic (line ends with '/ 0 /' and then 'ΚΑΕΚ 0' follows) – retain existing intent.
                        if (
                            (re.search(r'/\s*0\s*/\s*$', line) and re.search(r'ΚΑΕΚ\s*0', line))  # original (line-end) pattern
                            or (re.search(r'/\s*0\s*/\s*\|', line) and re.search(r'ΚΑΕΚ\s*0', line))  # cross-cell pattern: '/ 0 /' before a pipe
                        ):
                            original = original + '/0/0'
                        # if the line ALSO contains '/ 0 / 0' tokens separated, append
                        if re.search(r'/\s*0\s*/\s*0', line) and not original.endswith('/0/0'):
                            original = original + '/0/0'
                        break
    v = original
    if not v:
        return ""
    # If line(s) contain '/ 0 / 0' but value lacks it, attempt to append (scan limited window)
    if '/0/0' not in v:
        for line in raw_text.splitlines():
            if v[:10] in line and re.search(r'/\s*0\s*/\s*0', line):
                # ensure the base number matches
                if re.search(re.escape(v) + r'\s*/\s*0\s*/\s*0', line) or ('ΚΑΕΚ' in line):
                    v = v + '/0/0'
                    break
    # (Removed) Do not strip leading zeros here; handle equivalence at evaluation time.
    # Final tidy
    v = tidy(v)
    # Global scan: if no '/0/0' yet, but any line contains the exact base number and a fragmented '/ 0 / 0' pattern, append it.
    if '/0/0' not in v:
        base = v
        if base and '/' not in base:
            pattern = re.compile(re.escape(base) + r'.{0,40}/\s*0\s*/\s*0')
            pattern_rev = re.compile(r'/\s*0\s*/\s*0.{0,40}' + re.escape(base))
            for line in raw_text.splitlines():
                if pattern.search(line) or pattern_rev.search(line):
                    v = base + '/0/0'
                    break
    return v

def parse_pdfplumber_owners(text: str):
    owners = []
    if "Στοιχεία κυρίου του έργου" not in text:
        return owners
    section = text.split("Στοιχεία κυρίου του έργου",1)[1]
    for tok in ["Πρόσθετες", "Στοιχεία Διαγράμματος"]:
        if tok in section:
            section = section.split(tok,1)[0]
    lines = [l.rstrip() for l in section.splitlines() if l.strip()]
    # Stitch wrapped lines for right type
    merged: List[str] = []
    i=0
    while i < len(lines):
        if i+1 < len(lines) and lines[i].endswith("Πλήρης") and lines[i+1].startswith("κυριότητα"):
            merged.append(lines[i] + " " + lines[i+1])
            i += 2
        else:
            merged.append(lines[i])
            i += 1
    for ln in merged:
        if ln.startswith("Επώνυμο/ία"):
            continue
        toks = ln.split()
        # Expect pattern SURNAME GIVEN FATHER ROLE SHARE RIGHT ...
        if len(toks) < 6:
            continue
        # find share (0-100)
        share_idx = None
        for idx,t in enumerate(toks):
            try:
                v = float(t.replace(',','.'))
                if 0 <= v <= 100:
                    share_idx = idx
            except ValueError:
                pass
        if share_idx is None or share_idx+1 >= len(toks):
            continue
        right = " ".join(toks[share_idx+1:])
        owners.append({
            "Επώνυμο/ία": toks[0],
            "Όνομα": toks[1],
            "Ιδιότητα": "Ιδιοκτήτης",  # heuristic
            "Ποσοστό": float(toks[share_idx].replace(',','.')),
            "Τύπος δικαιώματος": right,
        })
    return owners

def extract_docling_tables(text: str):
    tables = []
    cur = []
    for line in text.splitlines():
        if line.strip().startswith("|"):
            cur.append(line)
        else:
            if cur:
                tables.append(cur)
                cur = []
    if cur:
        tables.append(cur)
    return tables

def parse_markdown_table(lines: List[str]):
    rows = []
    for l in lines:
        l = l.rstrip()
        if not l.strip().startswith("|"):
            continue
        parts = [c.strip() for c in l.strip().strip("|").split("|")]
        # skip delimiter row (all dashes)
        if all(re.fullmatch(r"-+", p) for p in parts):
            continue
        rows.append(parts)
    return rows

def parse_docling_owners(text: str):
    owners: List[Dict[str, object]] = []
    for tbl_lines in extract_docling_tables(text):
        tbl = parse_markdown_table(tbl_lines)
        if not tbl or len(tbl) < 2:
            continue
        # Some docling tables have a banner row (e.g., repeated "Στοιχεία κυρίου του έργου")
        # before the actual header. Find the first row that contains the essential columns.
        header_row_index = None
        for idx, row in enumerate(tbl[:3]):  # header usually within first 3 logical rows
            lowered = [c.lower() for c in row]
            if "επώνυμο/ία" in lowered and "ποσοστό" in lowered:
                header_row_index = idx
                header = lowered
                break
        if header_row_index is None:
            continue  # not an owners table
        # Data rows follow the detected header row
        data_rows = tbl[header_row_index + 1 :]
        for row in data_rows:
            if len(row) < len(header):
                # Skip malformed / truncated rows
                continue
            m = dict(zip(header, row))
            # Basic filters: require a surname or role keyword
            if not m.get("επώνυμο/ία", "").strip() and not m.get("ιδιότητα", "").strip():
                continue
            raw_share = str(m.get("ποσοστό", "")).strip()
            try:
                share_v = float(raw_share.replace(",", ".")) if raw_share else 0.0
            except ValueError:
                share_v = 0.0
            owners.append(
                {
                    "Επώνυμο/ία": m.get("επώνυμο/ία", "").strip(),
                    "Όνομα": m.get("όνομα", "").strip(),
                    "Ιδιότητα": m.get("ιδιότητα", "").strip(),
                    "Ποσοστό": share_v,
                    "Τύπος δικαιώματος": m.get("τύπος δικαιώματος", "").strip(),
                }
            )
    return owners

def parse_pdfplumber_coverage(text: str):
    if "Στοιχεία Διαγράμματος Κάλυψης" not in text:
        return {}
    section = text.split("Στοιχεία Διαγράμματος Κάλυψης",1)[1]
    lines = [l.strip() for l in section.splitlines() if l.strip()]
    cov = {}
    for ln in lines:
        for key in COVERAGE_KEYS:
            if ln.startswith(key):
                # Include optional leading minus for deltas (demolition cases)
                nums = re.findall(r"-?[0-9][0-9\.,]*", ln[len(key):])
                vals = [parse_eu_number(n) for n in nums[:4]]
                while len(vals) < 4:
                    vals.append(None)
                cov[key] = vals
    return cov

def parse_docling_coverage(text: str):
    cov = {}
    for tbl_lines in extract_docling_tables(text):
        tbl = parse_markdown_table(tbl_lines)
        if not tbl:
            continue
        # treat every row as potential data row if first cell matches a coverage key
        for i,row in enumerate(tbl):
            if len(row) == 5 and row[0] in COVERAGE_KEYS:
                # Skip if this looks like a header row (contains any Greek words for columns instead of numbers in row[1])
                if i == 0 and any(label in row[1] for label in ["ΥΦΙΣΤΑ", "ΝΟΜΙΜ", "ΠΡΑΓΜ", "ΣΥΝΟΛ"]):
                    continue
                cov[row[0]] = [parse_eu_number(c) for c in row[1:5]]
    return cov

def orient_coverage(cov_map: Dict[str, List[Optional[float]]]):
    # col order: ΥΦΙΣΤΑΜΕΝΑ, ΝΟΜΙΜΟΠΟΙΟΥΜΕΝΑ, ΠΡΑΓΜΑΤΟΠΟΙΟΥΜΕΝΑ, ΣΥΝΟΛΟ
    cols = ["ΥΦΙΣΤΑΜΕΝΑ","ΝΟΜΙΜΟΠΟΙΟΥΜΕΝΑ","ΠΡΑΓΜΑΤΟΠΟΙΟΥΜΕΝΑ","ΣΥΝΟΛΟ"]
    result = {c:{} for c in cols}
    for key in COVERAGE_KEYS:
        vals = cov_map.get(key, [None,None,None,None])
        for idx,c in enumerate(cols):
            v = vals[idx]
            result[c][key] = v if v is not None else 0.0
    return result

def build_structured(stem: str, raw_text: str, extractor: str):
    kaek = extract_kaek(raw_text) or ""
    # Fallback: if still empty and markdown tables exist, attempt structured scan
    if not kaek and '|' in raw_text:
        for l in raw_text.splitlines():
            if 'ΚΑΕΚ' not in l:
                continue
            if not l.strip().startswith('|'):
                continue
            cells = [c.strip() for c in l.strip().strip('|').split('|')]
            if not cells:
                continue
            # Normal orientation: find 'ΚΑΕΚ' then next cell is value
            for idx, c in enumerate(cells):
                if c == 'ΚΑΕΚ':
                    if idx + 1 < len(cells):
                        next_cell = cells[idx+1].strip()
                        cand = next_cell.split()[0] if next_cell else ''
                        cand_clean = cand.replace(' ', '')
                        if re.fullmatch(r'[0-9/]{6,}', cand_clean):
                            kaek = cand_clean
                            break
            if kaek:
                break
            # Reversed orientation: numeric first then 'ΚΑΕΚ' later
            for idx, c in enumerate(cells):
                if re.fullmatch(r'[0-9/]{6,}', c.replace(' ', '')) and any(cc == 'ΚΑΕΚ' for cc in cells[idx+1:]):
                    kaek = c.replace(' ', '')
                    break
            if kaek:
                break
    # Post-process KAEK to fix formatting issues (missing /0/0, leading zeros)
    kaek = _post_process_kaek(raw_text, kaek)
    if extractor == "pdfplumber":
        owners = parse_pdfplumber_owners(raw_text)
        cov = parse_pdfplumber_coverage(raw_text)
    else:
        owners = parse_docling_owners(raw_text)
        cov = parse_docling_coverage(raw_text)
    structured = {
        "ΑΔΑ": stem,
        "ΚΑΕΚ": kaek,
        "Στοιχεία κυρίου του έργου": owners,
        "Στοιχεία Διαγράμματος Κάλυψης": orient_coverage(cov),
    }
    return structured

def main():
    ap = argparse.ArgumentParser(description="Build structured JSON for permit PDFs from extracted text")
    ap.add_argument("--stem", help="Filename stem (without .pdf)")
    ap.add_argument("--pdf", type=Path, help="Path to PDF to infer stem", required=False)
    ap.add_argument("--compare-dir", type=Path, default=COMPARE_DIR, help="Directory holding *_pdfplumber.txt / *_docling.txt")
    ap.add_argument("--out-dir", type=Path, default=Path("debug/structured_json"), help="Output directory for structured JSON")
    ap.add_argument("--extractors", nargs="*", default=["pdfplumber", "docling"], help="Engines to run: pdfplumber, docling")
    ap.add_argument("--all", action="store_true", help="Process all stems present in compare-dir")
    ap.add_argument("--limit", type=int, default=0, help="Optional limit when using --all (0 = no limit)")
    args = ap.parse_args()
    # Normalize extractor list
    selected_extractors = [e.lower() for e in args.extractors]
    valid = {"pdfplumber", "docling"}
    for e in selected_extractors:
        if e not in valid:
            ap.error("--extractors must be subset of: pdfplumber docling")

    if args.all:
        if args.stem or args.pdf:
            ap.error("Don't combine --all with --stem/--pdf")
        if not args.compare_dir.exists():
            raise SystemExit(f"Compare dir not found: {args.compare_dir}")
        stems = sorted({p.name.rsplit('_pdfplumber.txt',1)[0] for p in args.compare_dir.glob('*_pdfplumber.txt')})
        if args.limit > 0:
            stems = stems[: args.limit]
        args.out_dir.mkdir(parents=True, exist_ok=True)
        count = 0
        for stem in stems:
            try:
                raws = load_raw(stem)
            except FileNotFoundError:
                print(f"Skipping {stem}: missing text files")
                continue
            for extractor, text in raws.items():
                if extractor not in selected_extractors:
                    continue
                data = build_structured(stem, text, extractor)
                path = args.out_dir / f"{stem}_{extractor}_structured.json"
                path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
            count += 1
            if count % 10 == 0:
                print(f"Processed {count} stems...")
        print(f"Done. Structured JSON created for {count} stems in {args.out_dir}")
        return

    if not args.stem and not args.pdf:
        ap.error("Provide --stem/--pdf or use --all")

    stem = args.stem or args.pdf.stem  # type: ignore
    # If PDF is provided, extract text on the fly for selected engines.
    if args.pdf:
        raws: Dict[str, str] = {}
        if "pdfplumber" in selected_extractors:
            try:
                raws["pdfplumber"] = _extract_text_pdfplumber(args.pdf)
            except Exception as e:
                print(f"Failed pdfplumber on {args.pdf.name}: {e}")
        if "docling" in selected_extractors:
            txt = _extract_text_docling(args.pdf)
            if txt is None:
                print("docling not available; skipping docling engine")
            else:
                raws["docling"] = txt
        if not raws:
            raise SystemExit("No text extracted; ensure selected engines are available")
    else:
        raws = load_raw(stem)

    out_dir = args.out_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    for extractor, text in raws.items():
        if extractor not in selected_extractors:
            continue
        data = build_structured(stem, text, extractor)
        path = out_dir / f"{stem}_{extractor}_structured.json"
        path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"Wrote {path}")

if __name__ == "__main__":  # pragma: no cover
    main()
