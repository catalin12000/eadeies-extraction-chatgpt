"""Produce structured JSON (one per extractor) for a given PDF stem.

Structure matches requested schema:
{
  "ΑΔΑ": <stem or detected>,
  "ΚΑΕΚ": <kaek>,
  "Στοιχεία κυρίου του έργου": [ { "Επώνυμο/ία": ..., "Όνομα": ..., ... } ],
  "Στοιχεία Διαγράμματος Κάλυψης": {
      "ΥΦΙΣΤΑΜΕΝΑ": { row: value, ... },
      "ΝΟΜΙΜΟΠΟΙΟΥΜΕΝΑ": {...},
      "ΠΡΑΓΜΑΤΟΠΟΙΟΥΜΕΝΑ": {...},
      "ΣΥΝΟΛΟ": {...}
  }
}

Input sources:
    A) debug/compare/<stem>_docling.txt (generated by compare_pdf_extractors.py --save-text)
    B) Or directly from a PDF via --pdf (docling engine)

Usage:
    # From pre-extracted texts (docling only)
    python build_structured_json.py --stem 63ΤΛ46Ψ842-Ξ2Μ

    # Directly from a PDF
    python build_structured_json.py --pdf data/Athens/63ΤΛ46Ψ842-Ξ2Μ.pdf --extractors docling
Outputs:
    debug/structured_json/<stem>_docling_structured.json
"""
from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Optional
import itertools

COMPARE_DIR = Path("debug/compare")

COVERAGE_KEYS = [
    "Εμβ. κάλυψης κτιρίου",
    "Εμβ. δόμησης κτιρίου",
    "Εμβ. ακάλυπτου χώρου οικοπέδου",
    "Όγκος κτιρίου (άνω εδάφους)",
    "Μέγιστο ύψος κτιρίου",
    "Αριθμός Ορόφων",
    "Αριθμός Θέσεων Στάθμευσης",
]

EU_NUM_RE = re.compile(r"^-?[0-9][0-9\.]*,[0-9]+$|^-?[0-9][0-9\.]*$")

def parse_eu_number(s: str) -> Optional[float]:
    """Parse a European-formatted number with support for optional leading minus.

    Rules:
      - Comma (,) is treated as decimal separator.
      - Dots (.) are thousands separators when a comma exists, OR when there are >=2 groups of 3 digits separated by dots.
      - When no comma exists and exactly one dot is present and the group after the dot has length 3, treat dot as thousands (e.g. '1.234' -> 1234) NOT decimal.
      - When no comma exists and a single dot is present with 1-2 trailing digits, treat as decimal (rare but allow if pattern clearly decimal like '7.5').
    """
    raw = s
    s = s.strip().replace(" ", "")
    if not s or s in {"-", "--"}:
        return None
    sign = -1 if s.startswith('-') else 1
    if s[0] in '+-':
        s_body = s[1:]
    else:
        s_body = s
    if not s_body:
        return None
    if ',' in s_body:
        int_part, dec = s_body.split(',', 1)
        int_part = int_part.replace('.', '')
        num_str = int_part + '.' + dec
    else:
        # No comma. Decide how to interpret dots.
        if s_body.count('.') == 0:
            num_str = s_body
        else:
            parts = s_body.split('.')
            if len(parts) > 1 and all(len(p) == 3 for p in parts[1:]):
                # All subsequent groups length 3: treat all dots as thousands
                num_str = ''.join(parts)
            elif len(parts) == 2 and len(parts[1]) == 3:
                # Single dot then 3 digits -> thousands
                num_str = parts[0] + parts[1]
            elif len(parts) == 2 and 1 <= len(parts[1]) <= 2:
                # Looks like a decimal
                num_str = parts[0] + '.' + parts[1]
            else:
                # Fallback: strip all dots (thousands assumption)
                num_str = ''.join(parts)
    try:
        return sign * float(num_str)
    except ValueError:
        return None

def load_raw(stem: str) -> Dict[str, str]:
    docling = (COMPARE_DIR / f"{stem}_docling.txt").read_text(encoding="utf-8")
    return {"docling": docling}

# --- Direct text extraction helpers (for programmatic use or --pdf flow) ---
# pdfplumber removed; docling-only

def _extract_text_docling(pdf_path: Path) -> Optional[str]:
    try:
        from docling.document_converter import DocumentConverter  # type: ignore
    except Exception:
        return None
    converter = DocumentConverter()
    result = converter.convert(str(pdf_path))
    text = result.document.export_to_text() if hasattr(result.document, "export_to_text") else str(result.document)
    return (text or "").strip()

def extract_pdf_to_structured(pdf_path: Path, extractor: str) -> Dict[str, object]:
    """High-level helper: extract one PDF using docling into structured JSON."""
    extractor = extractor.lower().strip()
    if extractor != "docling":
        raise ValueError("extractor must be 'docling'")
    raw_text = _extract_text_docling(pdf_path)
    if raw_text is None:
        raise RuntimeError("docling not installed or failed to import")
    return build_structured(pdf_path.stem, raw_text, extractor)

def extract_kaek(text: str) -> Optional[str]:
    """Extract ΚΑΕΚ value from raw text or markdown table rows.

    Handles patterns:
      ΚΑΕΚ <spaces> 0500...
      | ΚΑΕΚ | 0500... |
      ΚΑΕΚ : 0500...
    """
    # Direct simple pattern
    m = re.search(r"ΚΑΕΚ\s*[:\-]?\s*([0-9/]{6,})", text)
    if m:
        return m.group(1)
    # Scan line by line for table formatted rows
    for line in text.splitlines():
        if "ΚΑΕΚ" in line:
            m2 = re.search(r"ΚΑΕΚ[^0-9/]*([0-9/]{6,})", line)
            if m2:
                return m2.group(1)
    return None

def _post_process_kaek(raw_text: str, value: str) -> str:
    """Heuristically fix common KAEK extraction issues.

    Fixes implemented:
      1. Reconstruct missing '/0/0' when the source line clearly contains separated '/ 0 / 0'.
      2. Remove spurious leading zero for 11-digit numeric KAEK codes (e.g. '050097350003' -> '50097350003'),
         only when there are no slashes and pattern matches 0 + 5 + 10 digits.
      3. If no value yet, attempt secondary scan combining fragmented table cells where number appears before 'ΚΑΕΚ'.
    """
    # Helper to collapse internal spaces around slashes
    def tidy(v: str) -> str:
        v = v.replace(' /', '/').replace('/ ', '/').replace(' ', '')
        v = re.sub(r'/+', '/', v)
        return v

    original = value or ""

    # Secondary scan if empty: look for pattern like '([0-9]{6,}) / 0 / 0' or reversed table row
    if not original:
        for line in raw_text.splitlines():
            if 'ΚΑΕΚ' in line or re.search(r'[0-9]{6,}\s*/\s*0\s*/\s*0', line):
                m = re.search(r'([0-9]{6,})\s*/\s*0\s*/\s*0', line)
                if m:
                    original = m.group(1) + '/0/0'
                    break
                # Reversed orientation: numeric then '|' then 'ΚΑΕΚ'
                if 'ΚΑΕΚ' in line:
                    # collect all pure numeric >=6 chars in line
                    nums = re.findall(r'[0-9]{6,}', line)
                    if nums:
                        # choose longest
                        original = max(nums, key=len)
                        # Special fragmented pattern: we saw '/ 0 /' but final 0 is after 'ΚΑΕΚ'
                        # Case A: suffix fully fragmented across cells -> first cell ends with '/ 0 /' then a pipe, later cell has 'ΚΑΕΚ 0'
                        # Case B: earlier heuristic (line ends with '/ 0 /' and then 'ΚΑΕΚ 0' follows) – retain existing intent.
                        if (
                            (re.search(r'/\s*0\s*/\s*$', line) and re.search(r'ΚΑΕΚ\s*0', line))  # original (line-end) pattern
                            or (re.search(r'/\s*0\s*/\s*\|', line) and re.search(r'ΚΑΕΚ\s*0', line))  # cross-cell pattern: '/ 0 /' before a pipe
                        ):
                            original = original + '/0/0'
                        # if the line ALSO contains '/ 0 / 0' tokens separated, append
                        if re.search(r'/\s*0\s*/\s*0', line) and not original.endswith('/0/0'):
                            original = original + '/0/0'
                        break
    v = original
    if not v:
        return ""
    # If line(s) contain '/ 0 / 0' but value lacks it, attempt to append (scan limited window)
    if '/0/0' not in v:
        for line in raw_text.splitlines():
            if v[:10] in line and re.search(r'/\s*0\s*/\s*0', line):
                # ensure the base number matches
                if re.search(re.escape(v) + r'\s*/\s*0\s*/\s*0', line) or ('ΚΑΕΚ' in line):
                    v = v + '/0/0'
                    break
    # (Removed) Do not strip leading zeros here; handle equivalence at evaluation time.
    # Final tidy
    v = tidy(v)
    # Global scan: if no '/0/0' yet, but any line contains the exact base number and a fragmented '/ 0 / 0' pattern, append it.
    if '/0/0' not in v:
        base = v
        if base and '/' not in base:
            pattern = re.compile(re.escape(base) + r'.{0,40}/\s*0\s*/\s*0')
            pattern_rev = re.compile(r'/\s*0\s*/\s*0.{0,40}' + re.escape(base))
            for line in raw_text.splitlines():
                if pattern.search(line) or pattern_rev.search(line):
                    v = base + '/0/0'
                    break
    return v

# pdfplumber owners parsing removed

def extract_docling_tables(text: str):
    tables = []
    cur = []
    for line in text.splitlines():
        if line.strip().startswith("|"):
            cur.append(line)
        else:
            if cur:
                tables.append(cur)
                cur = []
    if cur:
        tables.append(cur)
    return tables

def parse_markdown_table(lines: List[str]):
    """Parse a markdown-like table, tolerating page-break splits.

    If a row appears truncated (e.g., fewer than 5 cells for coverage), attempt to merge
    with the next table line when it continues the row (starts without a header keyword).
    """
    rows: List[List[str]] = []
    buffer: Optional[List[str]] = None
    for l in lines:
        l = l.rstrip()
        if not l.strip().startswith("|"):
            continue
        parts = [c.strip() for c in l.strip().strip("|").split("|")]
        # skip delimiter row (all dashes)
        if all(re.fullmatch(r"-+", p) for p in parts):
            continue
        if buffer is not None:
            # merge continuation of previous row
            merged = buffer + parts
            rows.append(merged)
            buffer = None
            continue
        # detect likely truncated coverage row (first cell is a known key but columns < 5)
        if parts and parts[0] in COVERAGE_KEYS and len(parts) < 5:
            buffer = parts
            continue
        rows.append(parts)
    # flush buffer if any
    if buffer is not None:
        rows.append(buffer)
    return rows

def parse_docling_owners(text: str):
    owners: List[Dict[str, object]] = []
    for tbl_lines in extract_docling_tables(text):
        tbl = parse_markdown_table(tbl_lines)
        if not tbl or len(tbl) < 2:
            continue
        # Some docling tables have a banner row (e.g., repeated "Στοιχεία κυρίου του έργου")
        # before the actual header. Find the first row that contains the essential columns.
        header_row_index = None
        for idx, row in enumerate(tbl[:3]):  # header usually within first 3 logical rows
            lowered = [c.lower() for c in row]
            if "επώνυμο/ία" in lowered and "ποσοστό" in lowered:
                header_row_index = idx
                header = lowered
                break
        if header_row_index is None:
            continue  # not an owners table
        # Data rows follow the detected header row
        data_rows = tbl[header_row_index + 1 :]
        for row in data_rows:
            if len(row) < len(header):
                # Skip malformed / truncated rows
                continue
            m = dict(zip(header, row))
            # Basic filters: require a surname or role keyword
            if not m.get("επώνυμο/ία", "").strip() and not m.get("ιδιότητα", "").strip():
                continue
            raw_share = str(m.get("ποσοστό", "")).strip()
            try:
                share_v = float(raw_share.replace(",", ".")) if raw_share else 0.0
            except ValueError:
                share_v = 0.0
            owners.append(
                {
                    "Επώνυμο/ία": m.get("επώνυμο/ία", "").strip(),
                    "Όνομα": m.get("όνομα", "").strip(),
                    "Ιδιότητα": m.get("ιδιότητα", "").strip(),
                    "Ποσοστό": share_v,
                    "Τύπος δικαιώματος": m.get("τύπος δικαιώματος", "").strip(),
                }
            )
    return owners

# pdfplumber coverage parsing removed

def _parking_total_fallback(text: str) -> Optional[float]:
    """Try to recover ΣΥΝΟΛΟ for parking when the table row splits across pages.

    Heuristics:
      - Find the first occurrence of the key line.
      - Search within the same line and a small forward window for 'ΣΥΝΟΛ' label followed by an integer.
      - If not found, take the last integer in the line-snippet (common when columns are printed inline).
    """
    key = "Αριθμός Θέσεων Στάθμευσης"
    idx = text.find(key)
    if idx == -1:
        return None
    snippet = text[idx: idx + 400]
    # Prefer explicit ΣΥΝΟΛΟ label
    m = re.search(r"ΣΥΝΟΛ\S*[:|\s]+([0-9]{1,4})", snippet)
    if m:
        try:
            return float(m.group(1))
        except Exception:
            pass
    # Otherwise, collect integers in the first 2 lines after key and use the last
    lines = snippet.splitlines()[:3]
    ints: List[int] = []
    for ln in lines:
        for tok in re.findall(r"(?<![0-9])[0-9]{1,5}(?![0-9])", ln):
            try:
                ints.append(int(tok))
            except Exception:
                pass
    if ints:
        return float(ints[-1])
    return None

def parse_docling_coverage(text: str):
    cov = {}
    # Track whether we are in or near the coverage section and if we just saw the floors row
    in_coverage_context = False
    floors_seen_recently = False
    # Orphan numeric candidate captured immediately after floors
    orphan_candidate: Optional[List[Optional[float]]] = None
    for tbl_lines in extract_docling_tables(text):
        tbl = parse_markdown_table(tbl_lines)
        if not tbl:
            continue
        # Detect a coverage header within this table
        table_has_coverage_header = any(
            any(tok in cell for tok in ["ΥΦΙΣΤΑ", "ΝΟΜΙΜ", "ΠΡΑΓΜ", "ΣΥΝΟΛ"]) for row in tbl for cell in row
        )
        if table_has_coverage_header:
            in_coverage_context = True
            floors_seen_recently = False
        # treat every row as potential data row if first cell matches a coverage key
        for i,row in enumerate(tbl):
            if not row:
                continue
            # Skip if this looks like a banner/header row (column labels)
            if i <= 1 and len(row) > 1 and any(label in row[1] for label in ["ΥΦΙΣΤΑ", "ΝΟΜΙΜ", "ΠΡΑΓΜ", "ΣΥΝΟΛ"]):
                continue
            if row[0] in COVERAGE_KEYS:
                # Ensure we have 4 numeric cells; pad with None if truncated
                values = [parse_eu_number(c) for c in (row[1:5] + [None, None, None, None])[:4]]
                cov[row[0]] = values
                floors_seen_recently = (row[0] == "Αριθμός Ορόφων")
                continue
            # Heuristic: orphan numeric-only row just after floors -> likely the Parking row stripped of its label
            if in_coverage_context and floors_seen_recently:
                # Consider rows with exactly 4 numeric cells (integers most likely)
                numeric_cells = row[:4]
                if len(numeric_cells) == 4 and all(parse_eu_number(c) is not None for c in numeric_cells):
                    # Record a candidate to be applied later only if parking is missing or zero
                    if orphan_candidate is None:
                        orphan_candidate = [parse_eu_number(c) for c in numeric_cells]
                    floors_seen_recently = False  # consume the hint
                    continue
        # If table had no obvious coverage header, but follows immediately after a coverage table
        # and we haven't yet consumed the floors hint, try to match a single-row numeric table.
        if in_coverage_context and floors_seen_recently and tbl:
            # Some cases show the orphan numeric row as a tiny separate table
            first_row = tbl[0]
            numeric_cells = first_row[:4] if first_row else []
            if len(numeric_cells) == 4 and all(parse_eu_number(c) is not None for c in numeric_cells):
                if orphan_candidate is None:
                    orphan_candidate = [parse_eu_number(c) for c in numeric_cells]
                floors_seen_recently = False
    # Apply orphan candidate only if label-based extraction is missing or yields a zero total
    key = "Αριθμός Θέσεων Στάθμευσης"
    if orphan_candidate is not None:
        current = cov.get(key)
        use_candidate = False
        if current is None:
            use_candidate = True
        else:
            vals = (current + [None, None, None, None])[:4]
            if vals[3] is None or vals[3] == 0.0:
                use_candidate = True
        if use_candidate:
            cov[key] = [(v if v is not None else 0.0) for v in (orphan_candidate + [None, None, None, None])[:4]]

    # Final fallback: if parking total (ΣΥΝΟΛΟ) is still missing (None), try to recover from text around the key
    if key in cov:
        vals = cov.get(key) or [None, None, None, None]
        if len(vals) < 4:
            vals = (vals + [None, None, None, None])[:4]
        if vals[3] is None:
            tot = _parking_total_fallback(text)
            if tot is not None:
                vals[3] = tot
                cov[key] = vals
    else:
        # Entire row missing; if we can find a total, at least populate the ΣΥΝΟΛΟ column
        tot = _parking_total_fallback(text)
        if tot is not None:
            cov[key] = [0.0, 0.0, 0.0, tot]
    return cov

def orient_coverage(cov_map: Dict[str, List[Optional[float]]]):
    # col order: ΥΦΙΣΤΑΜΕΝΑ, ΝΟΜΙΜΟΠΟΙΟΥΜΕΝΑ, ΠΡΑΓΜΑΤΟΠΟΙΟΥΜΕΝΑ, ΣΥΝΟΛΟ
    cols = ["ΥΦΙΣΤΑΜΕΝΑ","ΝΟΜΙΜΟΠΟΙΟΥΜΕΝΑ","ΠΡΑΓΜΑΤΟΠΟΙΟΥΜΕΝΑ","ΣΥΝΟΛΟ"]
    result = {c:{} for c in cols}
    for key in COVERAGE_KEYS:
        vals = cov_map.get(key, [None,None,None,None])
        for idx,c in enumerate(cols):
            v = vals[idx]
            result[c][key] = v if v is not None else 0.0
    return result

def build_structured(stem: str, raw_text: str, extractor: str):
    kaek = extract_kaek(raw_text) or ""
    # Fallback: if still empty and markdown tables exist, attempt structured scan
    if not kaek and '|' in raw_text:
        for l in raw_text.splitlines():
            if 'ΚΑΕΚ' not in l:
                continue
            if not l.strip().startswith('|'):
                continue
            cells = [c.strip() for c in l.strip().strip('|').split('|')]
            if not cells:
                continue
            # Normal orientation: find 'ΚΑΕΚ' then next cell is value
            for idx, c in enumerate(cells):
                if c == 'ΚΑΕΚ':
                    if idx + 1 < len(cells):
                        next_cell = cells[idx+1].strip()
                        cand = next_cell.split()[0] if next_cell else ''
                        cand_clean = cand.replace(' ', '')
                        if re.fullmatch(r'[0-9/]{6,}', cand_clean):
                            kaek = cand_clean
                            break
            if kaek:
                break
            # Reversed orientation: numeric first then 'ΚΑΕΚ' later
            for idx, c in enumerate(cells):
                if re.fullmatch(r'[0-9/]{6,}', c.replace(' ', '')) and any(cc == 'ΚΑΕΚ' for cc in cells[idx+1:]):
                    kaek = c.replace(' ', '')
                    break
            if kaek:
                break
    # Post-process KAEK to fix formatting issues (missing /0/0, leading zeros)
    kaek = _post_process_kaek(raw_text, kaek)
    owners = parse_docling_owners(raw_text)
    cov = parse_docling_coverage(raw_text)
    # Meta diagnostics: detect if any markdown-like tables exist in the document
    try:
        tables = extract_docling_tables(raw_text)
        tables_count = len(tables)
        has_tables = tables_count > 0
    except Exception:
        tables_count = 0
        has_tables = False
    structured = {
        "ΑΔΑ": stem,
        "ΚΑΕΚ": kaek,
        "Στοιχεία κυρίου του έργου": owners,
        "Στοιχεία Διαγράμματος Κάλυψης": orient_coverage(cov),
        "_meta": {
            "extractor": extractor,
            "has_tables": has_tables,
            "tables_count": tables_count,
            "owners_count": len(owners),
            "coverage_keys_found": sorted(list(cov.keys())),
        },
    }
    return structured

def main():
    ap = argparse.ArgumentParser(description="Build structured JSON for permit PDFs from extracted text")
    ap.add_argument("--stem", help="Filename stem (without .pdf)")
    ap.add_argument("--pdf", type=Path, help="Path to PDF to infer stem", required=False)
    ap.add_argument("--compare-dir", type=Path, default=COMPARE_DIR, help="Directory holding *_docling.txt")
    ap.add_argument("--out-dir", type=Path, default=Path("debug/structured_json"), help="Output directory for structured JSON")
    ap.add_argument("--extractors", nargs="*", default=["docling"], help="Engines to run: docling")
    ap.add_argument("--all", action="store_true", help="Process all stems present in compare-dir")
    ap.add_argument("--limit", type=int, default=0, help="Optional limit when using --all (0 = no limit)")
    args = ap.parse_args()
    # Normalize extractor list
    selected_extractors = [e.lower() for e in args.extractors]
    valid = {"docling"}
    for e in selected_extractors:
        if e not in valid:
            ap.error("--extractors must be subset of: docling")

    if args.all:
        if args.stem or args.pdf:
            ap.error("Don't combine --all with --stem/--pdf")
        if not args.compare_dir.exists():
            raise SystemExit(f"Compare dir not found: {args.compare_dir}")
        stems = sorted({p.name.rsplit('_docling.txt',1)[0] for p in args.compare_dir.glob('*_docling.txt')})
        if args.limit > 0:
            stems = stems[: args.limit]
        args.out_dir.mkdir(parents=True, exist_ok=True)
        count = 0
        for stem in stems:
            try:
                raws = load_raw(stem)
            except FileNotFoundError:
                print(f"Skipping {stem}: missing text files")
                continue
            for extractor, text in raws.items():
                if extractor not in selected_extractors:
                    continue
                data = build_structured(stem, text, extractor)
                path = args.out_dir / f"{stem}_{extractor}_structured.json"
                path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
            count += 1
            if count % 10 == 0:
                print(f"Processed {count} stems...")
        print(f"Done. Structured JSON created for {count} stems in {args.out_dir}")
        return

    if not args.stem and not args.pdf:
        ap.error("Provide --stem/--pdf or use --all")

    stem = args.stem or args.pdf.stem  # type: ignore
    # If PDF is provided, extract text on the fly for selected engines.
    if args.pdf:
        raws: Dict[str, str] = {}
        if "docling" in selected_extractors:
            txt = _extract_text_docling(args.pdf)
            if txt is None:
                print("docling not available; skipping docling engine")
            else:
                raws["docling"] = txt
        if not raws:
            raise SystemExit("No text extracted; ensure selected engines are available")
    else:
        raws = load_raw(stem)

    out_dir = args.out_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    for extractor, text in raws.items():
        if extractor not in selected_extractors:
            continue
        data = build_structured(stem, text, extractor)
        path = out_dir / f"{stem}_{extractor}_structured.json"
        path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"Wrote {path}")

if __name__ == "__main__":  # pragma: no cover
    main()
